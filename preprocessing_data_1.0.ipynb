{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L1joPvfF-ykZ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import scipy\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import chardet\n",
        "with open('algal.csv', 'rb') as f:\n",
        "    enc = chardet.detect(f.read())  # or readline if the file is large\n",
        "\n",
        "df=pd.read_csv('algal.csv', encoding = enc['encoding'])"
      ],
      "metadata": {
        "id": "WEEpUPzRABWA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "394667f8-02a7-445d-b347-c03d20c78707"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'algal.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-af4c056153d7>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mchardet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'algal.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0menc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchardet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# or readline if the file is large\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'algal.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'algal.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "O2ewYAN9CJhJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "8mCeZ_-sBMja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df.describe()"
      ],
      "metadata": {
        "id": "MsKSL_gvBQX3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.dtypes\n",
        "\n",
        " #df.fillna(df.mean(), inplace=True)"
      ],
      "metadata": {
        "id": "TKj0jH3mLPiA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df)"
      ],
      "metadata": {
        "id": "SwxA_IbyO1_6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
        "df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].mean())\n",
        "\n",
        "# Handle null values for object columns\n",
        "object_cols = df.select_dtypes(include=['object']).columns\n",
        "df[object_cols] = df[object_cols].fillna(df[object_cols].mode().iloc[0])"
      ],
      "metadata": {
        "id": "seY4e0hVQDzB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.drop(columns=['Time of Occurrence'])"
      ],
      "metadata": {
        "id": "0USUM-4SQVJU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def remove_outliers(df, threshold=3):\n",
        "    \"\"\"\n",
        "    Remove outliers from a DataFrame\n",
        "\n",
        "    Parameters:\n",
        "        df (DataFrame): Input DataFrame\n",
        "        threshold (float): Z-score threshold for identifying outliers in numeric columns\n",
        "\n",
        "    Returns:\n",
        "        DataFrame: DataFrame with outliers removed\n",
        "    \"\"\"\n",
        "    # Copy the original DataFrame to avoid modifying the original data\n",
        "    cleaned_df = df.copy()\n",
        "\n",
        "    # Handle numeric columns\n",
        "    numeric_cols = cleaned_df.select_dtypes(include=['float64', 'int64']).columns\n",
        "    for col in numeric_cols:\n",
        "        z_scores = np.abs((cleaned_df[col] - cleaned_df[col].mean()) / cleaned_df[col].std())\n",
        "        outliers = cleaned_df[z_scores > threshold]\n",
        "        cleaned_df = cleaned_df.drop(outliers.index)\n",
        "\n",
        "    # Handle object columns\n",
        "    object_cols = cleaned_df.select_dtypes(include=['object']).columns\n",
        "    for col in object_cols:\n",
        "        # Define your own criteria for identifying outliers for object columns\n",
        "        # For example, remove rows with 'NaN' or specific strings\n",
        "        outliers = cleaned_df[cleaned_df[col].isin(['NaN', 'outlier_value'])]\n",
        "        cleaned_df = cleaned_df.drop(outliers.index)\n",
        "\n",
        "    return cleaned_df\n",
        "\n",
        "# Example usage:\n",
        "cleaned_df = remove_outliers(df)\n"
      ],
      "metadata": {
        "id": "HxFATiMOQlrv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer"
      ],
      "metadata": {
        "id": "KX-i-PeXSXvT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numeric_cols = cleaned_df.select_dtypes(include=['int64', 'float64']).columns\n",
        "object_cols = cleaned_df.select_dtypes(include=['object']).columns\n",
        "\n",
        "# Define preprocessing steps for numeric and object columns\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')),  # Handle missing values if any\n",
        "    ('scaler', StandardScaler())  # Standardize numeric features\n",
        "])\n",
        "\n",
        "object_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),  # Handle missing values if any\n",
        "    ('onehot', OneHotEncoder())  # One-hot encode categorical features\n",
        "])\n",
        "\n",
        "# Create preprocessing pipeline\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_cols),\n",
        "        ('cat', object_transformer, object_cols)])\n",
        "\n",
        "# Fit and transform the data\n",
        "processed_data = preprocessor.fit_transform(cleaned_df)\n",
        "\n",
        "# Get transformed numeric and object column names\n",
        "numeric_transformed_cols = preprocessor.transformers_[0][1]['scaler'].get_feature_names_out(numeric_cols)\n",
        "object_transformed_cols = preprocessor.transformers_[1][1]['onehot'].get_feature_names_out(object_cols)\n",
        "print(processed_data.shape)\n",
        "print(len(numeric_transformed_cols))\n",
        "print(len(object_transformed_cols))\n",
        "# Combine transformed numeric and object columns into a DataFrame\n",
        "# Combine transformed numeric and object columns into a DataFrame\n",
        "#processed_df = pd.DataFrame(processed_data, columns=list(numeric_transformed_cols) + list(object_transformed_cols.flatten()))\n",
        "# Combine transformed numeric and object columns into a DataFrame\n",
        "numeric_and_object_transformed_cols = list(numeric_transformed_cols) + list(object_transformed_cols.flatten())\n",
        "#processed_df = pd.DataFrame(processed_data, columns=numeric_and_object_transformed_cols)\n",
        "print(\"Numeric Transformed Columns Shape:\", numeric_transformed_cols.shape)\n",
        "print(\"Object Transformed Columns Shape:\", object_transformed_cols.shape)\n",
        "print(\"Shape of processed_data:\", processed_data.shape)\n",
        "print(\"Length of concatenated column names:\", len(numeric_and_object_transformed_cols))\n",
        "\n",
        "print(\"Column Names:\", processed_df.columns)\n",
        "# Create DataFrame from processed_data using ColumnTransformer\n",
        "processed_df = pd.DataFrame(processed_data, columns=numeric_and_object_transformed_cols)\n"
      ],
      "metadata": {
        "id": "x435Mf-_Rj-5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "column_names = cleaned_df.columns.tolist()\n",
        "\n",
        "print(\"Column Names:\", column_names)"
      ],
      "metadata": {
        "id": "Vyyi6DKUSfba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Processed Data Shape:\", processed_data.shape)\n",
        "print(\"Processed Data Sample:\")\n",
        "print(processed_data[:5])  # Print the first 5 rows of the processed data\n"
      ],
      "metadata": {
        "id": "f1e1gD4NWIIj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Shape of processed_data:\", processed_data.shape)\n",
        "print(\"Length of combined column names:\", len(numeric_and_object_transformed_cols))\n"
      ],
      "metadata": {
        "id": "csXcSeANWY0j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create DataFrame from processed_data\n",
        "processed_df = pd.DataFrame(processed_data)\n",
        "\n",
        "# Assign column names to the DataFrame\n",
        "processed_df.columns = numeric_and_object_transformed_cols\n"
      ],
      "metadata": {
        "id": "ETVzSZPPWfye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create DataFrame from processed_data with no column names initially\n",
        "processed_df = pd.DataFrame(processed_data, columns=None)\n",
        "\n",
        "# Assign column names to the DataFrame\n",
        "processed_df.columns = numeric_and_object_transformed_cols\n"
      ],
      "metadata": {
        "id": "0M_fWMI1WsR3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "\n",
        "# Load your DataFrame\n",
        "#cleaned_df = pd.read_csv(\"your_data.csv\")\n",
        "\n",
        "# Separate numerical and object columns\n",
        "numerical_cols = cleaned_df.select_dtypes(include=['float64', 'int64']).columns\n",
        "object_cols = cleaned_df.select_dtypes(include=['object']).columns\n",
        "\n",
        "# Convert object columns to numerical format\n",
        "for col in object_cols:\n",
        "    label_encoder = LabelEncoder()\n",
        "    cleaned_df[col] = label_encoder.fit_transform(cleaned_df[col])\n",
        "\n",
        "# Standardize numerical columns\n",
        "scaler_numerical = StandardScaler()\n",
        "cleaned_df[numerical_cols] = scaler_numerical.fit_transform(cleaned_df[numerical_cols])\n",
        "\n",
        "# Display standardized DataFrame\n",
        "print(cleaned_df.head())\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w6lbGEE1aIHY",
        "outputId": "d14e8761-04aa-4bc7-decd-1ad81fb37155"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Latitude  Longitude       SST  Chlorophyll a       par  Cells per litre  \\\n",
            "0  0.758882   2.537251 -2.968318      -0.345506 -1.435455        -0.374098   \n",
            "2  0.758882   0.886974 -0.015082      -1.833051  0.654619         0.105654   \n",
            "3  0.127591  -0.427070  0.536083      -0.007427 -0.619562        -0.457255   \n",
            "6  1.120805   0.586238 -0.054590       0.398267 -0.955994         1.544909   \n",
            "7 -3.304400  -0.383066  0.653476       0.195420  0.810569        -0.342115   \n",
            "\n",
            "   Day of year  Temperature  Dew point Temperature  Sea level pressure(hPa)  \\\n",
            "0     0.145822     0.132102               0.614062                -0.519737   \n",
            "2    -0.061151     0.662246              -0.472725                -0.519192   \n",
            "3     1.010238    -0.842217              -0.627980                -0.518022   \n",
            "6    -0.097676     0.733887              -2.232284                -0.518730   \n",
            "7    -2.009131    -0.355057              -0.330407                -0.520009   \n",
            "\n",
            "   ...     a_443     a_488     a_547     a_645     a_667     a_678    bb_443  \\\n",
            "0  ... -0.968924 -0.860604 -0.764843 -0.518280 -0.529231 -0.525673 -0.313513   \n",
            "2  ... -1.307003 -1.198811 -0.985433 -0.751071 -0.767549 -0.762084 -0.997875   \n",
            "3  ... -0.904752 -0.884208  4.148412 -0.629682 -0.656874 -0.653890 -0.500004   \n",
            "6  ...  0.672465  0.802795  0.776124  0.760345  0.765997  0.767915  0.804638   \n",
            "7  ... -0.766755 -0.621459 -0.569261 -0.284722 -0.285900 -0.282587 -0.422792   \n",
            "\n",
            "     bb_469   adg_443  angstrom  \n",
            "0 -0.336100 -0.062568 -0.722884  \n",
            "2 -0.784956 -0.151881 -1.040168  \n",
            "3 -0.453068  0.544554 -1.313042  \n",
            "6  0.596147  0.037954 -0.015111  \n",
            "7 -0.408700 -0.421825 -0.021110  \n",
            "\n",
            "[5 rows x 24 columns]\n"
          ]
        }
      ]
    }
  ]
}